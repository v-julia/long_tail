{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import operator\n",
    "import lightgbm as lgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "queries = {}\n",
    "with open('norm_queries.tsv', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "f.close()\n",
    "        \n",
    "titles= {}\n",
    "with open(\"norm_titles.tsv\", encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "f.close()\n",
    "        \n",
    "urls = {}\n",
    "with open('url.data', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line[:-1].split('\\t')\n",
    "    if line[1][:7] == 'http://':\n",
    "        line[1] = line[1][7:]\n",
    "    if line[1][:4] == 'www.':\n",
    "        line[1] = line[1][4:]\n",
    "    urls[line[0]] = line[1].split('/')[0]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train=[]\n",
    "with open(\"train.marks.tsv\", encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        train.append([line[0],line[1], line[2]])\n",
    "\n",
    "\n",
    "                \n",
    "test=[]\n",
    "with open(\"sample.csv\", encoding='utf-8') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        test.append([line[0],line[1],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in all_queries:\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        tfidf_score(q, (1,3), 'text_tfids_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf scores с ngram_range=(1,3) для текстов и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_feature(input_dir):\n",
    "    feature_dict = {}\n",
    "    files = os.listdir(input_dir)\n",
    "    for file in files:\n",
    "        with open(input_dir + '/' + file, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line[:-1].split('\\t')\n",
    "            feature_dict[(line[0], line[1])] = np.float64(line[2])\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tfids_13 = get_text_feature('text_tfids_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf scores с ngram_range=(3,7) для текстов и запросов text_tfids_37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfids_37 = get_text_feature('text_tfids_37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf scores с ngram_range=(1,3) для заголовков и запросов title_tfids_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tfids_13 = get_text_feature('title_tfids_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf scores с ngram_range=(3,8) для заголовков и запросов title_tfids_38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tfids_38 = get_text_feature('title_tfids_38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25Okapi score для текстов и запросов bm_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_texts = get_text_feature('bm_texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passage score для текстов и запросов, window=3, stride=1 pass_text_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_text_31 = get_text_feature('pass_text_31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passage score для текстов и запросов, window=3, stride=2 pass_text_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_text_32 = get_text_feature('pass_text_32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passage score для текстов и запросов, window=7, stride=3 pass_text_73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_text_73 = get_text_feature('pass_text_73')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passage score для заголовков и запросов, window=3, stride=1 pass_title_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_title_31 = get_text_feature('pass_title_31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use score для заголовков и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = get_text_feature('use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quse = get_text_feature('quse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2v score для заголовков и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = get_text_feature('d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext score для заголовков и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = get_text_feature('fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Списки похожих запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_queries = {}\n",
    "with open('similar_queries.txt', encoding='utf-8') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        spl = line[:-2].split('\\t')\n",
    "        if len(spl) == 1:\n",
    "            continue\n",
    "        tmp = spl[1].split(' ')\n",
    "        for i in range(13):\n",
    "            tmp.pop()\n",
    "        similar_queries[spl[0]] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поведенческие признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"sDBN/part-r-00000\" ,'r', encoding='utf-8')\n",
    "doc2sdbn = {}\n",
    "for line in file.readlines():\n",
    "    l=line[:-1].split('\\t')\n",
    "    k = l[0]\n",
    "    doc2sdbn[k]= np.array(l[1:], dtype = np.float64)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlfiles = os.listdir('url_q_features')\n",
    "uf = {}\n",
    "quf = {}\n",
    "for file in urlfiles:\n",
    "    with open('url_q_features/' + file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        line.pop()\n",
    "        if file[1] == 'q':\n",
    "            q = line.pop(0)\n",
    "            u = line.pop(0)\n",
    "            quf[(q,u)] = np.array(line, dtype = np.float64)\n",
    "        else:\n",
    "            u = line.pop(0)\n",
    "            uf[u] = np.array(line, dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file =open(\"q_features/part-r-00000\" ,'r', encoding='utf-8')\n",
    "qf = {}\n",
    "for line in file.readlines():\n",
    "    l=line[:-1].split('\\t')\n",
    "    k = l[0]\n",
    "    l.pop(0)\n",
    "    qf[k]= np.array(l, dtype = np.float64)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(cat):\n",
    "    global similar_queries\n",
    "    x = []\n",
    "    y = []\n",
    "    qid = []\n",
    "    index = []\n",
    "    for entry in tqdm.tqdm(cat):\n",
    "        try:\n",
    "            # sDBN\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,doc2sdbn[entry[1]]))\n",
    "            except:\n",
    "                temp = None\n",
    "                if temp is None:\n",
    "                    [temp] = [[0.0] * 3]\n",
    "                tempx = np.hstack((tempx,temp))\n",
    "            # query features\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,qf[entry[0]]))\n",
    "            except:\n",
    "                temp = None\n",
    "                if temp is None:\n",
    "                    for q in similar_queries[entry[0]]:\n",
    "                        if q in qf.keys():\n",
    "                            temp = qf[q]\n",
    "                            break\n",
    "                if temp is None:\n",
    "                    [temp] = [[0.0] * 23]\n",
    "                tempx = np.hstack((tempx,temp))\n",
    "            # url features\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,uf[entry[1]]))\n",
    "            except:\n",
    "                temp = None\n",
    "                \n",
    "                for q in similar_queries[entry[0]]:\n",
    "                    if entry[1] in uf.keys():\n",
    "                        temp = uf[entry[1]]\n",
    "                        break\n",
    "                if temp is None:\n",
    "                    [temp] = [[0.0] * 13]\n",
    "                tempx = np.hstack((tempx,temp))\n",
    "            # query-url features\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,quf[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                temp = None\n",
    "\n",
    "                for q in similar_queries[entry[0]]:\n",
    "                    if (q,entry[1]) in quf.keys():\n",
    "                        temp = quf[(q,entry[1])]\n",
    "                        break\n",
    "                if temp is None:\n",
    "                    [temp] = [[0.0] * 43]\n",
    "                tempx = np.hstack((tempx,temp))\n",
    "\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,text_tfids_13[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,text_tfids_37[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,title_tfids_13[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,title_tfids_38[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,bm_texts[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,pass_text_31[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,pass_text_32[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,pass_text_73[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,pass_title_31[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,use[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,quse[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,d2v[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "            try:\n",
    "                tempx = np.hstack((tempx,fasttext[(entry[0],entry[1])]))\n",
    "            except:\n",
    "                tempx = np.hstack((tempx,np.array([0.0])))\n",
    "\n",
    "            x.append(tempx)\n",
    "            y.append(float(entry[2]))\n",
    "            qid.append(int(entry[0]))\n",
    "            index.append((entry[0], entry[1]))\n",
    "        except:\n",
    "            continue\n",
    "    X = np.row_stack([row for row in x])\n",
    "    Y = np.row_stack([row for row in y])\n",
    "    return X, Y, qid, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, qid_train, _ = create_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test, _ , index = create_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_unique = np.unique(qid_train, return_counts=True)[1]\n",
    "lgbm_ranker = lgb.LGBMRanker(objective='lambdarank', max_depth=12, n_estimators= 1000, subsample=0.8, learning_rate=0.01, num_leaves=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm_ranker.fit(X_train,Y_train.ravel(),group=qids_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = lgbm_ranker.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm_ranker.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_pred):\n",
    "    query_doc = {}\n",
    "    query_doc_score = {}\n",
    "    \n",
    "    with open('sample.csv', 'r', encoding='UTF-8') as f:\n",
    "        sample = f.readlines()\n",
    "\n",
    "        for i in range(1,len(sample)):\n",
    "\n",
    "            l = sample[i][:-1].split(',')\n",
    "            if l[0] not in query_doc.keys():\n",
    "                query_doc[l[0]] = []\n",
    "            else:\n",
    "                query_doc[l[0]].append(l[1])\n",
    "            query_doc_score[tuple(l)] = y_pred[i-1]\n",
    "    results = []\n",
    "    for query in query_doc:\n",
    "        res = {}\n",
    "        for doc in query_doc[query]:\n",
    "            try:\n",
    "                res[doc] = query_doc_score[(query,doc)]\n",
    "            except:\n",
    "                continue\n",
    "        res_sorted = sorted(res.items(), key = operator.itemgetter(1), reverse = True)\n",
    "        for i in range(len(res_sorted)):\n",
    "            results.append([query,res_sorted[i][0]])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = create_submission(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit_tr.csv', 'w') as fout:\n",
    "    fout.write('QueryId,DocumentId\\n')\n",
    "    for result in results:\n",
    "        fout.write(result[0] + ',' + result[1]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
