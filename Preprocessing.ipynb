{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymystem3 import Mystem\n",
    "from pyaspeller import YandexSpeller\n",
    "import pymorphy2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = YandexSpeller()\n",
    "mystem = Mystem(grammar_info=False)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_words = {}\n",
    "def norm_form(string):\n",
    "    words = string.split(' ')\n",
    "    words_norm = []\n",
    "    for word in words:\n",
    "        if word in dict_words:\n",
    "            words_norm.append(dict_words[word])\n",
    "        else:\n",
    "            norm = morph.parse(word)[0].normal_form\n",
    "            words_norm.append(norm)\n",
    "            dict_words[word] = norm\n",
    "    return ' '.join(words_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправим запросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = []\n",
    "queries = {}\n",
    "with open('queries.tsv', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        l = line.strip('\\n').split('\\t')\n",
    "        fixed = speller.spelled(l[1])\n",
    "        fixed = norm_form(fixed)\n",
    "        queries[l[0]] = fixed\n",
    "        new_lines.append('\\t'.join([l[0], fixed]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_q_files = open('norm_queries_raw.tsv', 'w')\n",
    "norm_q_files.writelines(new_lines)\n",
    "norm_q_files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*norm_queries.txt* - это *norm_queries_raw.txt* после проверки глазками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "queries = {}\n",
    "with open('norm_queries.tsv', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        queries[line[0]] = line[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим только те документы, которые есть в трейне и тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs=[]\n",
    "train_queries = []\n",
    "with open(\"train.marks.tsv\", 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        line=line.strip('\\n').split('\\t')\n",
    "        train_docs.append(line[1])\n",
    "        train_queries.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = list(set(train_docs))\n",
    "train_queries = list(set(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs=[]\n",
    "test_queries = []\n",
    "with open(\"sample.csv\", 'r', encoding='utf-8') as f:\n",
    "    f.readline()\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n').split(',')\n",
    "        test_queries.append(line[0])\n",
    "        test_docs.append(line[1])\n",
    "test_docs = list(set(test_docs))\n",
    "test_queries = list(set(test_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = train_queries + test_queries\n",
    "all_docs = train_docs + test_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем документы в отдельные папки, заодно отдельно сохраняем заголовки, чтобы потом исправить в них опечатки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "     os.mkdir('data')\n",
    "except:\n",
    "     n = None\n",
    "        \n",
    "titles = {}\n",
    "        \n",
    "with open(\"docs.tsv\", encoding='utf-8') as docs_file:\n",
    "    for line in tqdm(docs_file):\n",
    "        l = line.split(\"\\t\")\n",
    "        if l[0] in all_docs:\n",
    "            titles[l[0]] = l[1]\n",
    "            #with open('data/'+l[0]+'.txt','w',encoding='utf-8') as file:\n",
    "            #    file.write('\\t'.join(l[1:]))\n",
    "            #file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправляем опечатки в заголовках и приводим слова к нормальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_lines = []\n",
    "\n",
    "for doc_id in tqdm(titles.keys()):\n",
    "    fixed = speller.spelled(titles[doc_id])\n",
    "    fixed = norm_form(fixed)\n",
    "    titles_lines.append(doc_id + '\\t' + fixed + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_titles = open('norm_titles.tsv', 'w')\n",
    "f_titles.writelines(titles_lines)\n",
    "f_titles.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
