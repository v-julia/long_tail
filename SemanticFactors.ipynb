{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import fasttext\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "with open('norm_queries.tsv', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "titles= {}\n",
    "with open(\"norm_titles.tsv\" ,'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В словарях сначала все query_id для трейна, потом для теста, как они идут по порядку в train.marks.tsv, sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2docid = {}\n",
    "qid2query = {}\n",
    "docid2title = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_unq = []\n",
    "with open(\"train.marks.tsv\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n').split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            if line[0] not in qid2docid:\n",
    "                qid2docid[line[0]] = []\n",
    "            qid2docid[line[0]].append(line[1])\n",
    "            docid2title[line[1]] = titles[line[1]]\n",
    "            if line[0] not in train_queries_unq:\n",
    "                qid2query[line[0]] = queries[line[0]]\n",
    "                train_queries_unq.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries_unq = []\n",
    "with open(\"sample.csv\", 'r', encoding='utf-8') as f:\n",
    "    f.readline()\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n').split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            if line[0] not in qid2docid:\n",
    "                qid2docid[line[0]] = []\n",
    "            qid2docid[line[0]].append(line[1])\n",
    "            docid2title[line[1]] = titles[line[1]]\n",
    "            if line[0] not in test_queries_unq:\n",
    "                qid2query[line[0]] = queries[line[0]]\n",
    "                test_queries_unq.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = list(qid2docid.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large USE embeddings\n",
    "\n",
    "Посчитаем косинусное рассстояние между эмбеддингами large use для заголовков и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large USE\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_score(q, out_dir):\n",
    "\n",
    "    global qid2query\n",
    "    global docid2title\n",
    "    global embed\n",
    "    \n",
    "    \n",
    "    raw_titles = []\n",
    "    docs = []\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            raw_titles.append(docid2title[doc])\n",
    "            docs.append(doc)\n",
    "        except:\n",
    "            print(q, doc)\n",
    "            \n",
    "    q_emb = embed([qid2query[q]]).numpy()\n",
    "    t_embs = embed(raw_titles).numpy()\n",
    "\n",
    "\n",
    "    with open(out_dir+'/{}.txt'.format(q),'w') as fout:\n",
    "        for i,doc in enumerate(docs):\n",
    "            fout.write(str(q) + '\\t' +str(doc) + '\\t' + str(cos(q_emb,t_embs[i])) + '\\n')\n",
    "\n",
    "    del t_embs\n",
    "    del q_emb\n",
    "    del raw_titles\n",
    "    del docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'use'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        use_score(q,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec embeddings\n",
    "\n",
    "Тоже на заголовках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_score(key, outdir):\n",
    "\n",
    "    global qid2docid\n",
    "    global docid2title\n",
    "    global qid2query\n",
    "\n",
    "    max_epochs = 100\n",
    "    \n",
    "    docs = []\n",
    "    titles = []\n",
    "    \n",
    "    for docid in qid2docid[key]:\n",
    "        docs.append(docid)\n",
    "        titles.append(docid2title[docid])\n",
    "    tagged_docs = [TaggedDocument(word_tokenize(title),[docid]) for docid, title in zip(docs, titles)]\n",
    "    tagged_docs.append(TaggedDocument(qid2query[key],[9999999]))\n",
    "    \n",
    "    d2v = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "    d2v.build_vocab(tagged_docs)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        d2v.train(tagged_docs,\n",
    "        total_examples=d2v.corpus_count,\n",
    "        epochs=dv.iter)\n",
    "        d2v.alpha -= 0.0002\n",
    "        d2v.min_alpha = dv.alpha\n",
    "\n",
    "    with open(outdir+'/{}.txt'.format(key),'w') as fout:\n",
    "        for docid in qid2docid[key]:\n",
    "            fout.write(key + '\\t' + docid + '\\t' + str(cos(d2v.docvecs[9999999],d2v.docvecs[docid]))+'\\n')\n",
    "    return key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dir = 'd2v'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in tqdm(all_queries[:3]):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        doc2vec_score(q,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText embeddings\n",
    "\n",
    "Используем модель для русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('ru', if_exists='ignore')\n",
    "ft_model = fasttext.load_model('cc.ru.300.bin')\n",
    "def fasttext_score(q, out_dir):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    global docid2title\n",
    "    global ft_model\n",
    "    cur_titles = {}\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            cur_titles[doc] = ft_model.get_word_vector(docid2title[doc])\n",
    "        except:\n",
    "            continue\n",
    "    cur_titles['q'] = ft_model.get_word_vector(qid2query[q])\n",
    "    with open(out_dir + '/{}.txt'.format(q),'w') as fout:\n",
    "        for outdoc in cur_titles.keys():\n",
    "            if outdoc != 'q':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(cur_titles[outdoc],cur_titles['q'])) + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dir = 'fasttext'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in tqdm(all_queries[:3]):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        fasttext_score(q,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схожесть d2v эмбеддингов запросов\n",
    "\n",
    "Будет применять для сглаживания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_tagged = []\n",
    "for qid in qid2query.keys():\n",
    "    queries_tagged.append(TaggedDocument(qid2query[qid],[qid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "alpha = 0.0025\n",
    "d2v = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "d2v.build_vocab(queries_tagged)\n",
    "for epoch in range(max_epochs):\n",
    "    d2v.train(queries_tagged,\n",
    "    total_examples=d2v.corpus_count,\n",
    "    epochs=d2v.iter)\n",
    "    d2v.alpha -= 0.0002\n",
    "    d2v.min_alpha = d2v.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_q(qid):\n",
    "    global d2v\n",
    "    global all_queries\n",
    "    \n",
    "    distances = {}\n",
    "    for q in all_queries:\n",
    "        if q != qid:\n",
    "            distances[q] = cos(d2v.docvecs[qid],d2v.docvecs[q])\n",
    "    distances_sorted = sorted(distances.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    sim_top = []\n",
    "    for i in range(15):\n",
    "        sim_top.append(distances_sorted[i][0])\n",
    "\n",
    "    line = '\\t'.join([qid]+sim_top)+'\\n'\n",
    "    \n",
    "    return line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for q in all_queries[:3]:\n",
    "    lines.append(get_similar_q(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sim = open('similar_queries.txt', 'w')\n",
    "file_sim.writelines(lines)\n",
    "file_sim.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
