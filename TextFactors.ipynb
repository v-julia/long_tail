{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем запросы, заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "with open('norm_queries.tsv', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "titles= {}\n",
    "with open(\"norm_titles.tsv\" ,'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В словарях сначала все query_id для трейна, потом для теста, как они идут по порядку в train.marks.tsv, sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2docid = {}\n",
    "qid2query = {}\n",
    "docid2title = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_unq = []\n",
    "with open(\"train.marks.tsv\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n').split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            if line[0] not in qid2docid:\n",
    "                qid2docid[line[0]] = []\n",
    "            qid2docid[line[0]].append(line[1])\n",
    "            docid2title[line[1]] = titles[line[1]]\n",
    "            if line[0] not in train_queries_unq:\n",
    "                qid2query[line[0]] = queries[line[0]]\n",
    "                train_queries_unq.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries_unq = []\n",
    "with open(\"sample.csv\", 'r', encoding='utf-8') as f:\n",
    "    f.readline()\n",
    "    for line in f.readlines():\n",
    "        line=line.strip('\\n').split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            if line[0] not in qid2docid:\n",
    "                qid2docid[line[0]] = []\n",
    "            qid2docid[line[0]].append(line[1])\n",
    "            docid2title[line[1]] = titles[line[1]]\n",
    "            if line[0] not in test_queries_unq:\n",
    "                qid2query[line[0]] = queries[line[0]]\n",
    "                test_queries_unq.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = list(qid2docid.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Косинусное расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    sp1 = sp1.todense()\n",
    "    sp2 = sp2.todense()\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf для текстов документов и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_score(q, ngr_range, outdir):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    tf = TfidfVectorizer(analyzer = 'char', ngram_range=ngr_range)\n",
    "    corpus = [qid2query[q]]\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "             with open('data/{}.txt'.format(doc), encoding='utf-8') as f:\n",
    "                corpus.append(f.readline())\n",
    "        except:\n",
    "             continue\n",
    "\n",
    "    tfidf_vectors = tf.fit_transform(corpus)\n",
    "    result_lines = []\n",
    "    for i in range(1,len(corpus)):\n",
    "        line = '\\t'.join([q, qid2docid[q][i-1], str(cos(tfidf_vectors[i],tfidf_vectors[0]))]) + '\\n'\n",
    "        result_lines.append(line)\n",
    "    with open(outdir+'/{}.txt'.format(q),'w') as fout:\n",
    "        fout.writelines(result_lines)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'text_tfids_13'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in all_queries:\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        tfidf_score(q, (1,3), 'text_tfids_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'text_tfids_37'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in all_queries:\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        tfidf_score(q, (3,7), 'text_tfids_37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf для заголовков и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_title_score(q, ngr_range, out_dir):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    global docid2title\n",
    "\n",
    "    tf = TfidfVectorizer(analyzer = 'char', ngram_range=ngr_range)\n",
    "    corpus = []\n",
    "    docs = []\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            corpus.append(docid2title[doc])\n",
    "            docs.append(doc)\n",
    "        except:\n",
    "            continue\n",
    "    corpus.append(qid2query[q])\n",
    "    tfidf_vectors = tf.fit_transform(corpus)\n",
    "\n",
    "    with open(out_dir+'/{}.txt'.format(q),'w') as fout:\n",
    "        for i, doc_id in enumerate(docs):\n",
    "            fout.write(str(doc_id)+ '\\t'+str(cos(tfidf_vectors[i],tfidf_vectors[-1])) + '\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'title_tfids_13'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        tfidf_title_score(q, (1,3), 'title_tfids_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'title_tfids_38'\n",
    "try:\n",
    "     os.mkdir(out_dir)\n",
    "except:\n",
    "     n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        tfidf_title_score(q, (3,8), 'title_tfids_38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 для текстов и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_text_score(q, out_dir):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    corpus = []\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            with open('data/{}.txt'.format(doc), encoding='utf-8') as f:\n",
    "                corpus.append([doc,word_tokenize(f.readline())[:1000]])\n",
    "        except:\n",
    "            continue\n",
    "    bm = BM25Okapi([doc for idd, doc in corpus])\n",
    "    scores = bm.get_scores(qid2query[q])\n",
    "    with open(out_dir+'/'+str(q)+'.txt','w') as fout:\n",
    "        for i, score in enumerate(scores):\n",
    "            fout.write(str(corpus[i][0]) + '\\t' + str(score) + '\\n')\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:44<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'bm_texts'\n",
    "try:\n",
    "    os.mkdir('bm_texts')\n",
    "except:\n",
    "    n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        BM25_text_score(q, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пассажи\n",
    "\n",
    "для текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def passage_score(q, out_dir, window, stride):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    \n",
    "    texts_dict = {}\n",
    "    idf_dict = {}\n",
    "    \n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            with open('data/{}.txt'.format(doc), encoding='utf-8') as f:\n",
    "                texts_dict[doc] = word_tokenize(f.readline())\n",
    "                for word in texts_dict[doc]:\n",
    "                    if word not in idf_dict.keys():\n",
    "                        idf_dict[word] = 0\n",
    "                    idf_dict[word] += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for key in idf_dict.keys():\n",
    "        idf_dict[key] = len(texts_dict) / idf_dict[key]\n",
    "\n",
    "    query_words = word_tokenize(qid2query[q])\n",
    "    query_set = set(query_words)\n",
    "    with open(out_dir+'/'+q+'.txt','w') as fout:\n",
    "        for key in texts_dict.keys():\n",
    "            cur_text = texts_dict[key]\n",
    "            pas_score = 0\n",
    "            for i in range(0, len(cur_text) - window, stride):\n",
    "                scut = set(cur_text[i:i+window])\n",
    "                inters = set.intersection(scut, query_set)\n",
    "                if len(inters) > 0:\n",
    "                    cidf = 0\n",
    "                    for w in inters:\n",
    "                        cidf+=idf_dict[w]\n",
    "                    for w in inters:\n",
    "                        for k in inters:\n",
    "                            if w != k:\n",
    "                                if (cur_text.index(w) - cur_text.index(k))*(query_words.index(w) - query_words.index(k)) > 0:\n",
    "                                    cidf *= 1.05\n",
    "                                if abs((cur_text.index(w) - cur_text.index(k)))<=abs((query_words.index(w) - query_words.index(k))):\n",
    "                                    cidf *= 1.001\n",
    "                    pas_score+=cidf*(len(cur_text) - window - i)\n",
    "            fout.write(str(key) + '\\t' + str(pas_score) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'pass_text_31'\n",
    "try:\n",
    "    os.mkdir('pass_text_31')\n",
    "except:\n",
    "    n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        passage_score(q, out_dir, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.88s/it]\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'pass_text_32'\n",
    "try:\n",
    "    os.mkdir('pass_text_32')\n",
    "except:\n",
    "    n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        passage_score(q, out_dir, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.79s/it]\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'pass_text_73'\n",
    "try:\n",
    "    os.mkdir('pass_text_73')\n",
    "except:\n",
    "    n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        passage_score(q, out_dir, 7, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_score_title(q, out_dir, window, stride):\n",
    "    global qid2docid\n",
    "    global qid2query\n",
    "    global docid2title\n",
    "    \n",
    "    texts_dict = {}\n",
    "    idf_dict = {}\n",
    "    for doc in qid2docid[q]:\n",
    "        try:\n",
    "            texts_dict[doc] = word_tokenize(docid2title[doc])\n",
    "            for w in texts_dict[doc]:\n",
    "                if w not in idf_dict.keys():\n",
    "                    idf_dict[w] = 0\n",
    "                idf_dict[w] += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for key in idf_dict.keys():\n",
    "        idf_dict[key] = len(texts_dict) / idf_dict[key]\n",
    "\n",
    "    query_words = word_tokenize(qid2query[q])\n",
    "    query_set = set(query_words)\n",
    "    with open(out_dir+'/'+q+'.txt','w') as fout:\n",
    "        for key in texts_dict.keys():\n",
    "            cur_text = texts_dict[key]\n",
    "            pas_score = 0\n",
    "            for i in range(0, len(cur_text) - window, stride):\n",
    "                scut = set(cur_text[i:i+window])\n",
    "                inters = set.intersection(scut, query_set)\n",
    "                if len(inters) > 0:\n",
    "                    cidf = 0\n",
    "                    for w in inters:\n",
    "                        cidf+=idf_dict[w]\n",
    "                    for w in inters:\n",
    "                        for k in inters:\n",
    "                            if w != k:\n",
    "                                if (cur_text.index(w) - cur_text.index(k))*(query_words.index(w) - query_words.index(k)) > 0:\n",
    "                                    cidf *= 1.05\n",
    "                                if abs((cur_text.index(w) - cur_text.index(k)))<=abs((query_words.index(w) - query_words.index(k))):\n",
    "                                    cidf *= 1.01\n",
    "                    pas_score+=cidf*(len(cur_text) - window - i)\n",
    "            fout.write(str(key) + '\\t' + str(pas_score) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 97.57it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'pass_title_31'\n",
    "try:\n",
    "    os.mkdir('pass_title_31')\n",
    "except:\n",
    "    n = None\n",
    "\n",
    "for q in tqdm(all_queries):\n",
    "    if q + '.txt' not in os.listdir(out_dir):\n",
    "        passage_score_title(q, out_dir, 3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
